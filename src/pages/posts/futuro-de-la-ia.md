---
layout: ../../layouts/PostLayout.astro
title: '¬øC√≥mo aprend√≠ 2 frameworks web en una semana?üöÄ'
pubDate: 2023-09-20
description: 'Mi experiencia de aprendizaje con 2 incre√≠bles nuevas tecnolog√≠as para m√≠.'
image:
    url: 'https://i0.wp.com/css-tricks.com/wp-content/uploads/2021/05/astro-homepage.png?fit=2396%2C1192&ssl=1'
    alt: 'The full Astro logo.'
tags: ["astro", "blogging", "learning in public", "tailwind"]
posted: true
---

√çndice

- [ü§î Por qu√© aprender√≠a?](#-por-qu√©-aprender√≠a)
- [üëÅÔ∏è Aprendizaje activo](#Ô∏è-aprendizaje-activo)
  - [Astro](#astro)
  - [Tailwind](#tailwind)
  - [Resultado final](#resultado-final)
- [üòê Conclusi√≥n](#-conclusi√≥n)
- [‚ù§Ô∏è What's next](#Ô∏è-whats-next)

Which is AI's future?

La IA ha tenido avances grand√≠simos en los √∫ltimos 2 a√±os, denota una tendencia exponencial (pruebas), aqu√≠ analizaremos el camino que est√° tomando esta tecnolog√≠a y hacia donde se dirige.

## Estado actual

Actualmente las IAs han demostrado una grandisima capacidad de realizar tareas que requieren algun tipo de razonamiento humano, en especial las IAs generativas se han destacado, como ChatGPT o dall-e o SORA, las cuales muestran estar a la par o ser incluso mejores que los humanos en tareas espec√≠ficas, como generar texto, im√°genes o video.

Es el problema actual, de hecho lo que buscan empresas como OpenAI es lograr una IA cap√°z de hacer cualquier tarea, una inteligencia artificial general (AGI), lo cual se va logrando poco a poco empezando con los modelos multimodales como GPT4-V o Gemini, los cuales en un solo modelo puedes tanto procesar texto como im√°genes.

## AGI - todo

## ASI

La AGI definitivamente no ser√° el paso final, el libro "Superinteligencia" de x nos propuso sabiamente diferentes maneras en la que podemos desarrollar una inteligencia sumamente mayor a la inteligencia humana actual. La m√°s probable y la que evidenciaremos ser√° la superinteligencia artificial (ASI), una IA que es superior a un humano en todos los aspectos, ya sea en velocidad de razonamiento, mayor calidad de razonamiento o razonamiento colectivo.

acumular mas infor, recordar mas

## Que pasar√° con la ASI

Cuando tengamos un sistema m√°s inteligente que nosotros mismos es natural pensar que este tendr√° una capacidad mayor para desarrollar sistemas a√∫n mejores que el mismo, dejandonos a los humanos obsoletos. Es por eso y dem√°s que los lideres de la IA como Sam Altman han insistido tanto en regular sus inventos, quiz√°s la velocidad sea un problema mas que algo bueno.

velocidad extraordinaria

S
upongamos que apareciera un agente superinteligente digital, y que, por alguna
raz√≥n, quisiera controlar el mundo: ¬øSer√≠a capaz de hacerlo? 

gobierno global?

La raz√≥n principal de que la humanidad domine la Tierra es que nuestros cerebros
tienen un conjunto ligeramente m√°s amplio de facultades que otros animales.1

frikis

ia con ci de 6.455: entonces, ¬øqu√©?

superinteligencia -> dise√±ar estrategias



primera estrategia, crear vision, desarrollar modelo de negocio, analizar mercado ver oportunidades y tomar decisiones referente a ellas



---

This document reflects the strategy we‚Äôve refined over the past two years, including feedback from many people internal and external to OpenAI. The timeline to AGI remains uncertain, but our Charter will guide us in acting in the best interests of humanity throughout its development.

OpenAI‚Äôs mission is to ensure that artificial general intelligence (AGI)‚Äîby which we mean highly autonomous systems that outperform humans at most economically valuable work‚Äîbenefits all of humanity. We will attempt to directly build safe and beneficial AGI, but will also consider our mission fulfilled if our work aids others to achieve this outcome. To that end, we commit to the following principles:

Broadly distributed benefits
We commit to use any influence we obtain over AGI‚Äôs deployment to ensure it is used for the benefit of all, and to avoid enabling uses of AI or AGI that harm humanity or unduly concentrate power.

Our primary fiduciary duty is to humanity. We anticipate needing to marshal substantial resources to fulfill our mission, but will always diligently act to minimize conflicts of interest among our employees and stakeholders that could compromise broad benefit.

Long-term safety
We are committed to doing the research required to make AGI safe, and to driving the broad adoption of such research across the AI community.

We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be ‚Äúa better-than-even chance of success in the next two years.‚Äù

Technical leadership
To be effective at addressing AGI‚Äôs impact on society, OpenAI must be on the cutting edge of AI capabilities‚Äîpolicy and safety advocacy alone would be insufficient.

We believe that AI will have broad societal impact before AGI, and we‚Äôll strive to lead in those areas that are directly aligned with our mission and expertise.

Cooperative orientation
We will actively cooperate with other research and policy institutions; we seek to create a global community working together to address AGI‚Äôs global challenges.

We are committed to providing public goods that help society navigate the path to AGI. Today this includes publishing most of our AI research, but we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research.

---

Our mission is to ensure that artificial general intelligence‚ÄîAI systems that are generally smarter than humans‚Äîbenefits all of humanity.

If AGI is successfully created, this technology could help us elevate humanity by increasing abundance, turbocharging the global economy, and aiding in the discovery of new scientific knowledge that changes the limits of possibility.

AGI has the potential to give everyone incredible new capabilities; we can imagine a world where all of us have access to help with almost any cognitive task, providing a great force multiplier for human ingenuity and creativity.

On the other hand, AGI would also come with serious risk of misuse, drastic accidents, and societal disruption. Because the upside of AGI is so great, we do not believe it is possible or desirable for society to stop its development forever; instead, society and the developers of AGI have to figure out how to get it right.A
[A]
We seem to have been given lots of gifts relative to what we expected earlier: for example, it seems like creating AGI will require huge amounts of compute and thus the world will know who is working on it, it seems like the original conception of hyper-evolved RL agents competing with each other and evolving intelligence in a way we can‚Äôt really observe is less likely than it originally seemed, almost no one predicted we‚Äôd make this much progress on pre-trained language models that can learn from the collective preferences and output of humanity, etc.

AGI could happen soon or far in the future; the takeoff speed from the initial AGI to more powerful successor systems could be slow or fast. Many of us think the safest quadrant in this two-by-two matrix is short timelines and slow takeoff speeds; shorter timelines seem more amenable to coordination and more likely to lead to a slower takeoff due to less of a compute overhang, and a slower takeoff gives us more time to figure out empirically how to solve the safety problem and how to adapt.


Although we cannot predict exactly what will happen, and of course our current progress could hit a wall, we can articulate the principles we care about most:

We want AGI to empower humanity to maximally flourish in the universe. We don‚Äôt expect the future to be an unqualified utopia, but we want to maximize the good and minimize the bad, and for AGI to be an amplifier of humanity.
We want the benefits of, access to, and governance of AGI to be widely and fairly shared.
We want to successfully navigate massive risks. In confronting these risks, we acknowledge that what seems right in theory often plays out more strangely than expected in practice. We believe we have to continuously learn and adapt by deploying less powerful versions of the technology in order to minimize ‚Äúone shot to get it right‚Äù scenarios.
The short term
There are several things we think are important to do now to prepare for AGI.

First, as we create successively more powerful systems, we want to deploy them and gain experience with operating them in the real world. We believe this is the best way to carefully steward AGI into existence‚Äîa gradual transition to a world with AGI is better than a sudden one. We expect powerful AI to make the rate of progress in the world much faster, and we think it‚Äôs better to adjust to this incrementally.

A gradual transition gives people, policymakers, and institutions time to understand what‚Äôs happening, personally experience the benefits and downsides of these systems, adapt our economy, and to put regulation in place. It also allows for society and AI to co-evolve, and for people collectively to figure out what they want while the stakes are relatively low.

We currently believe the best way to successfully navigate AI deployment challenges is with a tight feedback loop of rapid learning and careful iteration. Society will face major questions about what AI systems are allowed to do, how to combat bias, how to deal with job displacement, and more. The optimal decisions will depend on the path the technology takes, and like any new field, most expert predictions have been wrong so far. This makes planning in a vacuum very difficult.B
[B]
For example, when we first started OpenAI, we didn‚Äôt expect scaling to be as important as it has turned out to be. When we realized it was going to be critical, we also realized our original structure wasn‚Äôt going to work‚Äîwe simply wouldn‚Äôt be able to raise enough money to accomplish our mission as a nonprofit‚Äîand so we came up with a new structure.

As another example, we now believe we were wrong in our original thinking about openness, and have pivoted from thinking we should release everything (though we open source some things, and expect to open source more exciting things in the future!) to thinking that we should figure out how to safely share access to and benefits of the systems. We still believe the benefits of society understanding what is happening are huge and that enabling such understanding is the best way to make sure that what gets built is what society collectively wants (obviously there‚Äôs a lot of nuance and conflict here).


Generally speaking, we think more usage of AI in the world will lead to good, and want to promote it (by putting models in our API, open-sourcing them, etc.). We believe that democratized access will also lead to more and better research, decentralized power, more benefits, and a broader set of people contributing new ideas.

As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models. Our decisions will require much more caution than society usually applies to new technologies, and more caution than many users would like. Some people in the AI field think the risks of AGI (and successor systems) are fictitious; we would be delighted if they turn out to be right, but we are going to operate as if these risks are existential.

At some point, the balance between the upsides and downsides of deployments (such as empowering malicious actors, creating social and economic disruptions, and accelerating an unsafe race) could shift, in which case we would significantly change our plans around continuous deployment.

As our systems get closer to AGI, we are becoming increasingly cautious with the creation and deployment of our models.

Second, we are working towards creating increasingly aligned and steerable models. Our shift from models like the first version of GPT-3 to InstructGPT and ChatGPT is an early example of this.

In particular, we think it‚Äôs important that society agree on extremely wide bounds of how AI can be used, but that within those bounds, individual users have a lot of discretion. Our eventual hope is that the institutions of the world agree on what these wide bounds should be; in the shorter term we plan to run experiments for external input. The institutions of the world will need to be strengthened with additional capabilities and experience to be prepared for complex decisions about AGI.

The ‚Äúdefault setting‚Äù of our products will likely be quite constrained, but we plan to make it easy for users to change the behavior of the AI they‚Äôre using. We believe in empowering individuals to make their own decisions and the inherent power of diversity of ideas.

We will need to develop new alignment techniques as our models become more powerful (and tests to understand when our current techniques are failing). Our plan in the shorter term is to use AI to help humans evaluate the outputs of more complex models and monitor complex systems, and in the longer term to use AI to help us come up with new ideas for better alignment techniques.

Importantly, we think we often have to make progress on AI safety and capabilities together. It‚Äôs a false dichotomy to talk about them separately; they are correlated in many ways. Our best safety work has come from working with our most capable models. That said, it‚Äôs important that the ratio of safety progress to capability progress increases.

Third, we hope for a global conversation about three key questions: how to govern these systems, how to fairly distribute the benefits they generate, and how to fairly share access.

In addition to these three areas, we have attempted to set up our structure in a way that aligns our incentives with a good outcome. We have a clause in our Charter about assisting other organizations to advance safety instead of racing with them in late-stage AGI development. We have a cap on the returns our shareholders can earn so that we aren‚Äôt incentivized to attempt to capture value without bound and risk deploying something potentially catastrophically dangerous (and of course as a way to share the benefits with society). We have a nonprofit that governs us and lets us operate for the good of humanity (and can override any for-profit interests), including letting us do things like cancel our equity obligations to shareholders if needed for safety and sponsor the world‚Äôs most comprehensive UBI experiment.

We have attempted to set up our structure in a way that aligns our incentives with a good outcome.

We think it‚Äôs important that efforts like ours submit to independent audits before releasing new systems; we will talk about this in more detail later this year. At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models. We think public standards about when an AGI effort should stop a training run, decide a model is safe to release, or pull a model from production use are important. Finally, we think it‚Äôs important that major world governments have insight about training runs above a certain scale.

The long term
We believe that the future of humanity should be determined by humanity, and that it‚Äôs important to share information about progress with the public. There should be great scrutiny of all efforts attempting to build AGI and public consultation for major decisions.

The first AGI will be just a point along the continuum of intelligence. We think it‚Äôs likely that progress will continue from there, possibly sustaining the rate of progress we‚Äôve seen over the past decade for a long period of time. If this is true, the world could become extremely different from how it is today, and the risks could be extraordinary. A misaligned superintelligent AGI could cause grievous harm to the world; an autocratic regime with a decisive superintelligence lead could do that too.

AI that can accelerate science is a special case worth thinking about, and perhaps more impactful than everything else. It‚Äôs possible that AGI capable enough to accelerate its own progress could cause major changes to happen surprisingly quickly (and even if the transition starts slowly, we expect it to happen pretty quickly in the final stages). We think a slower takeoff is easier to make safe, and coordination among AGI efforts to slow down at critical junctures will likely be important (even in a world where we don‚Äôt need to do this to solve technical alignment problems, slowing down may be important to give society enough time to adapt).

Successfully transitioning to a world with superintelligence is perhaps the most important‚Äîand hopeful, and scary‚Äîproject in human history. Success is far from guaranteed, and the stakes (boundless downside and boundless upside) will hopefully unite all of us.

We can imagine a world in which humanity flourishes to a degree that is probably impossible for any of us to fully visualize yet. We hope to contribute to the world an AGI aligned with such flourishing.

Footnotes
We seem to have been given lots of gifts relative to what we expected earlier: for example, it seems like creating AGI will require huge amounts of compute and thus the world will know who is working on it, it seems like the original conception of hyper-evolved RL agents competing with each other and evolving intelligence in a way we can‚Äôt really observe is less likely than it originally seemed, almost no one predicted we‚Äôd make this much progress on pre-trained language models that can learn from the collective preferences and output of humanity, etc.

AGI could happen soon or far in the future; the takeoff speed from the initial AGI to more powerful successor systems could be slow or fast. Many of us think the safest quadrant in this two-by-two matrix is short timelines and slow takeoff speeds; shorter timelines seem more amenable to coordination and more likely to lead to a slower takeoff due to less of a compute overhang, and a slower takeoff gives us more time to figure out empirically how to solve the safety problem and how to adapt.‚Ü©Ô∏é

For example, when we first started OpenAI, we didn‚Äôt expect scaling to be as important as it has turned out to be. When we realized it was going to be critical, we also realized our original structure wasn‚Äôt going to work‚Äîwe simply wouldn‚Äôt be able to raise enough money to accomplish our mission as a nonprofit‚Äîand so we came up with a new structure.

As another example, we now believe we were wrong in our original thinking about openness, and have pivoted from thinking we should release everything (though we open source some things, and expect to open source more exciting things in the future!) to thinking that we should figure out how to safely share access to and benefits of the systems. We still believe the benefits of society understanding what is happening are huge and that enabling such understanding is the best way to make sure that what gets built is what society collectively wants (obviously there‚Äôs a lot of nuance and conflict here).‚Ü©Ô∏é

Authors
Sam Altman

---

Each year since 2012, the world has seen a new step function advance in AI capabilities. Though these advances are across very different fields like vision (2012), simple video games (2013), machine translation (2014), complex board games (2015), speech synthesis (2016), image generation (2017), robotic control (2018), and writing text (2019), they are all powered by the same approach: innovative applications of deep neural networks coupled with increasing computational power. But still, AI system building today involves a lot of manual engineering for each well-defined task.

In contrast, an AGI will be a system capable of mastering a field of study to the world-expert level, and mastering more fields than any one human‚Äîlike a tool which combines the skills of Curie, Turing, and Bach. An AGI working on a problem would be able to see connections across disciplines that no human could. We want AGI to work with people to solve currently intractable multi-disciplinary problems, including global challenges such as climate change, affordable and high-quality healthcare, and personalized education. We think its impact should be to give everyone economic freedom to pursue what they find most fulfilling, creating new opportunities for all of our lives that are unimaginable today.

Openai Team Offsite 2019
OpenAI team and their families at our July 2019 offsite.
OpenAI is producing a sequence of increasingly powerful AI technologies, which requires a lot of capital for computational power. The most obvious way to cover costs is to build a product, but that would mean changing our focus. Instead, we intend to license some of our pre-AGI technologies, with Microsoft becoming our preferred partner for commercializing them.

We believe that the creation of beneficial AGI will be the most important technological development in human history, with the potential to shape the trajectory of humanity. We have a hard technical path in front of us, requiring a unified software engineering and AI research effort of massive computational scale, but technical success alone is not enough. To accomplish our mission of ensuring that AGI (whether built by us or not) benefits all of humanity, we‚Äôll need to ensure that AGI is deployed safely and securely; that society is well-prepared for its implications; and that its economic upside is widely shared. If we achieve this mission, we will have actualized Microsoft and OpenAI‚Äôs shared value of empowering everyone.

Read press release
Wat

---

Given the picture as we see it now, it‚Äôs conceivable that within the next ten years, AI systems will exceed expert skill level in most domains, and carry out as much productive activity as one of today‚Äôs largest corporations.

In terms of both potential upsides and downsides, superintelligence will be more powerful than other technologies humanity has had to contend with in the past. We can have a dramatically more prosperous future; but we have to manage risk to get there. Given the possibility of existential risk, we can‚Äôt just be reactive. Nuclear energy is a commonly used historical example of a technology with this property; synthetic biology is another example.

We must mitigate the risks of today‚Äôs AI technology too, but superintelligence will require special treatment and coordination.

A starting point
There are many ideas that matter for us to have a good chance at successfully navigating this development; here we lay out our initial thinking on three of them.

First, we need some degree of coordination among the leading development efforts to ensure that the development of superintelligence occurs in a manner that allows us to both maintain safety and help smooth integration of these systems with society. There are many ways this could be implemented; major governments around the world could set up a project that many current efforts become part of, or we could collectively agree (with the backing power of a new organization like the one suggested below) that the rate of growth in AI capability at the frontier is limited to a certain rate per year.

And of course, individual companies should be held to an extremely high standard of acting responsibly.

Second, we are likely to eventually need something like an IAEA for superintelligence efforts; any effort above a certain capability (or resources like compute) threshold will need to be subject to an international authority that can inspect systems, require audits, test for compliance with safety standards, place restrictions on degrees of deployment and levels of security, etc. Tracking compute and energy usage could go a long way, and give us some hope this idea could actually be implementable. As a first step, companies could voluntarily agree to begin implementing elements of what such an agency might one day require, and as a second, individual countries could implement it. It would be important that such an agency focus on reducing existential risk and not issues that should be left to individual countries, such as defining what an AI should be allowed to say.

Third, we need the technical capability to make a superintelligence safe. This is an open research question that we and others are putting a lot of effort into.

What‚Äôs not in scope
We think it‚Äôs important to allow companies and open-source projects to develop models below a significant capability threshold, without the kind of regulation we describe here  (including burdensome mechanisms like licenses or audits).

Today‚Äôs systems will create tremendous value in the world and, while they do have risks, the level of those risks feel commensurate with other Internet technologies and society‚Äôs likely approaches seem appropriate.

By contrast, the systems we are concerned about will have power beyond any technology yet created, and we should be careful not to water down the focus on them by applying similar standards to technology far below this bar.

Public input and potential
But the governance of the most powerful systems, as well as decisions regarding their deployment, must have strong public oversight. We believe people around the world should democratically decide on the bounds and defaults for AI systems. We don't yet know how to design such a mechanism, but we plan to experiment with its development. We continue to think that, within these wide bounds, individual users should have a lot of control over how the AI they use behaves.

Given the risks and difficulties, it‚Äôs worth considering why we are building this technology at all.

At OpenAI, we have two fundamental reasons. First, we believe it‚Äôs going to lead to a much better world than what we can imagine today (we are already seeing early examples of this in areas like education, creative work, and personal productivity). The world faces a lot of problems that we will need much more help to solve; this technology can improve our societies, and the creative ability of everyone to use these new tools is certain to astonish us. The economic growth and increase in quality of life will be astonishing.

Second, we believe it would be unintuitively risky and difficult to stop the creation of superintelligence. Because the upsides are so tremendous, the cost to build it decreases each year, the number of actors building it is rapidly increasing, and it‚Äôs inherently part of the technological path we are on, stopping it would require something like a global surveillance regime, and even that isn‚Äôt guaranteed to work. So we have to get it right.